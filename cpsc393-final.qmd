---
title: "CPSC 393 Final Project Technical Report"
author: "Karen Ngo, Michelle Zhang, Todd Hartog"
format: pdf
---

# Introduction
This technical report explores the usage of deep learning models in classifying brain tumor (MRI) scans.

## Background, Problem, and Motivation
**Brain tumors** are one of the most aggressive diseases among children and adults. Roughly **85-90%** of all primary Central Nervous System (CNS) tumors are brain tumors which makes it an extremely important medical issue to research and learn more about. In the United States, roughly 25,000 people are diagnosed with brain tumors every year and roughly 100,000 people worldwide.The survival rates are very poor for cancerous brain tumors with some varying between 12 to 18 months. The 5-year survival rate is predicted to be roughly **36%** for females and **34%** for males which is very low compared to other diseases and illnesses.

The urgency of this situation sparked and drove the motivation and spark for this project. Manual examination can be error-prone leading to false diagnoses which can produce deadly consequences. Finding ways to accurately detect brain tumors through the use of data and machine learning can be incredibly powerful as technology keeps evolving to become more accurate and reliable.

### MRI Scans
The best technique for detecting brain tumors is through **Magnetic Resonance Imaging (MRI)**. An MRI is a medical imaging technique that uses magnetic fields and radio waves to generated detailed images of inside the body. Below is an example of an MRI scan with a brain tumor with the tumor circled in red.

![MRI Scan of the Brain](/Users/michellezhang/Desktop/mri.png){width=300}

## Project Goals
The goal for this project is to build a **Convolutional Neural Network (CNN) Model** to classify MRI scans as showing a tumor or not as well as the type of tumor. Additionally, we strive to fine tune the model so that its accuracy is high enough that it could be useful in the real world.

## Ethical Concerns
As with many machine learning projects, it is crucial to acknowledge the ethical concerns and address each of those issues.

### Patient Privacy and Consent
MRI scans allow radiologists to see into a patient's head which is very sensitive information. Therefore, patient privacy and consent is important. With the further enhancement of technology, patient anonymization and security may be at risk for being able to identify patients based on their brain scans. It is important to keep this sensitive information about the patient's health and medical history protected.

### High-risk Classification
This is considered a high-risk classification meaning that false positives and negatives can have dangerous consequences and in this case, life-threatening consequences. Since brain tumors are considered to be very aggressive diseases that need to be taken extremely seriously, having wrong classifications can lead to unwanted outcomes. For instance, a false negative means that a patient had a brain tumor and the model predicts that they do not or the model predicts the wrong type of tumor. As seen in the table describing the tumors, the treatments and severities differ between tumor which means this wrong diagnoses may lead a patient to not receive the right treatment or any treatment at all.

### Algorithm Bias
Due to no metadata or further information about the patients, algorithm bias may occur meaning the demographics of the patients may lead to biased or unhelpful results. Not having a diverse enough patient pool may result in the model predicting certain demographics better than others which would result in poor model performance for certain unseen data.

## Data Cleaning
The data cleaning process was straightforward for this project. Since the data was pre-split into training and testing, the image sizes varied. Therefore, the only cleaning done on the data was making sure all of the image sizes were consistent which was 150px by 150px.

# Analysis
This section contains the analysis performed on the data to understand it better before feeding it through the models.

## Data Overview
The dataset is taken from [Kaggle](https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri/data?select=Training) and is titled Brain Tumor Classification (MRI). It contains a total of **3,264** images total with **2,870** images in the **Training** folder and **394** in the **Testing** folder which is roughly a **90/10** train test split.

In each of the folders, there are subfolders containing images from each of the classes of tumors. The types of tumors in the dataset are the following:

| Tumor | Cancerous | Appearance | Treatment |
|--------------------|-----------|---------------------------------|---------------------------|
| Glioma Tumor | Yes | Areas of contrast enhancement, increased vascularity, indistinct borders, can infiltrate surrounding brain tissue | Surgical removal, radiation therapy, chemotherapy |
| Meningioma Tumor | No | Well-defined, round, lobulated masses, usually have dural tail (tapering extension), usually constrast-enhancing | Careful monitoring for low-risk tumors, surgery and radiation therapy for higher-risk cases |
| Pituitary Tumor | No | Usually located within the sella turcica (a bony structure at the base of the skull where the pituitary gland is) | Medication; surgery and radiation therapy if medicine is ineffective |
| No Tumor | No | no abnormal masses, consistent signals, brain anatomy clear | No need |

As seen in the table above, the shapes of the tumors are all relatively different which can be picked up in the MRI scan. Additionally, the treatments of each of the tumors differ and vary in severity. Therefore, it is crucial to be aware of the impact of incorrect classifications.

## Tumor Visualizations
Paired with the descriptions of the tumors above, below are MRI scans of each tumor for a better visualization of the them.

### Glioma Tumor
Below is an example of an MRI with a Glioma tumor with the tumor circled in red.

\newpage
![MRI Scan of a Glioma Tumor](/Users/michellezhang/Desktop/glioma.png){width=200}

Glioma tumors originate in the glial cells of the brain and can occur in various parts of the brain such as both cerebral hemispheres, cerebellum, and brainstem.

High-grade gliomas appear to be irregular and have enhanced patterns that contrast with the colors of the MRI scans surrounding the tumor which makes them more easily detected. Low-grade gliomas are less distinct in an MRI scan and can be more difficult to detect.

In the training data, the glioma scans vary grately in size, shape, and color. There are some that are very clear such as the one shown above, and others that are more hidden and therefore more difficult to detect.

### Meningioma Tumor
Below is an example of an MRI with a Meningioma tumor with the tumor circled in red.

\newpage
![MRI Scan of a Meningioma Tumor](/Users/michellezhang/Desktop/meningioma.png){width=200}

Meningioma tumors originate from meninges which are the protective outer layers covering the brain and the spinal cord. This tumor is commonly found near the surface of the brain and attached to the meninges.

These tumors are usually well-defined in the MRI scan and appear to be round masses with trailing tails. These tumors are easier to detect than most.

Looking at all of the training images, these tumors seem to be the most obvious to detect and most occur on the edge of the brain as shown in the image above.

### Pituitary Tumor
Below is an example of an MRI with a Pituitary tumor with the tumor circled in red.

![MRI Scan of a Pituitary Tumor](/Users/michellezhang/Desktop/pituitary.png){width=200}

Pituitary tumors originate from the pituitary gland which is a small gland at the base of the brain near the sella turcica, a bony structure in that area.

These tumors are usually located in the center of the brain near the optic nerves. These tumors are usually easier to detect than most tumors. As seen in the image above, this description aligns with the MRI scans found in the training dataset.

### No Tumor
Below is an example of an MRI with no tumor.

![MRI Scan of No Tumor](/Users/michellezhang/Desktop/no-tumor.png){width=200}

An MRI scan without a tumor shows the different regions of the brain with some of the brain definition preserved in the image shown by the different colors. The colors are usually separated by gray matter which is composed of nerve cell bodies and white matter which contains nerve fibers and axons. The cerebral ventricles, cerebellum, brainstem, and major blood vessels shown in the scan. There should be no abnormal masses or growths shown with different coloring and shapes that is expected.

## MRI Scan Angles
The MRI scans came in different angles of the head which is important to acknowledge as this inconsistency may be relevant to explain the results later on.

# Methods
This section highlights the details about the three models in this technical report, one being a **Convolutional Neural Network (CNN)** and the other two being a **pre-trained models**.

## Convolutional Neural Network (CNN) Background
A convolutional neural network (CNN) is a neural network used for processing spatial data, in this case, images with the goal of learning hierarchical feature representation. 

/newpage
![CNN](/Users/michellezhang/Desktop/cnn.png){width=400}

**Convolutional layers** are layers at the core of the network. These layers apply filters to the data in order to comprehend it using hierarchical feature representation to learn complex features about the image.

**Pooling layers** are layers that are used to downsample the dimensions to make the size of the image consistent throughout the network. A common way to do this is **max pooling** which retains the maximum value in that region.

**Activation functions** allow the processing of the data in our model to learn meaningful complex patterns and relationships in the data. There are many different types of functions, but the ones commonly used in non-linear modeling like CNN networks are **ReLu**. They are commonly used for deeper layers of the network and especially with convolutional layers which is consistent with our model architecture. Another common function is **Sigmoid** which is often used in the output of a binary classification to produce probabilities. The output layer of our model uses Sigmoid activation.

**Dropout** is a form of *regularization* that randomly drops a specified fraction of the nodes to prevent the model from being overly reliant on certain nodes which reduces *overfitting*.

**Flattening layers** are used to convert multi-dimensional data into one-dimensional data in order to reshape the data in between layers so it can be fed into later layers.

**Loss functions** are chosen based on the desired output. In our case, the loss function chosen is *categorical crossentropy* which is for categorical outputs.

**Optimizer** find the optimal parameters that can make the model perform the best for its given task. It accomplishes this by minimizing the loss function, using gradient descent for some, adjusting the learning rate which determines the size of the steps taken during parameter updates, momentum, regularization, etc. We chose the *Adam* optimizer which combines the mean square propagation and momentum.

**Early stopping** is another regularization technique used in our CNN to prevent *overfitting* by stopping the training process once the performance starts to degrade at a certain point. It consistently monitoring the model's performance to assess when the training should stop.

## Convolutional Neural Network (CNN) Model Architecture
The Convolutional Neural Network had the following architecture:

1. 4 Convolutional layers
2. 7 Activation layers (ReLu for all and Sigmoid for the last)
3. 2 Pooling layers
4. 1 Flattening layer
5. 3 Dropout layers (rate of 0.3)
6. Adam as optimizer (learning rate of 0.0001)
7. Utilizes early stopping (validation loss with patience of 0.5)

## Transfer Learning Models Background
Transfer learning model start with a pre-trained neural network model that had been trained on large amounts of data. Then, the final layer of the pre-trained model would be added for the specific task. This model differs from CNNs because most of the model is pre-trained on other data. The weights are learned based on that data. Typically, a CNN is trained from scratch which has its own pros and cons. Transfer learning model tend to be more complex because of its extension training on prior data.

Common transfer learning models are VGG, ResNet, Inception, MobileNet, Xception, etc.
In this technical report, we use VGG16 and EfficientNetB3.

## VGG16 Transfer Learning Model Architecture
This pre-trained model is a **VGG16 Transfer Leaning Model**. This model is short for Visual Geometry Group and the 16 represents that it is a 16-layer model. It has a deep CNN architecture and is known for its strong performance on image classification and for its simplicity and uniform architecture.

Below are the characteristics of the last few layers of the model:

1. 4 Dense layers
2. 3 Activation layers (ReLu for all and Sigmoid for the last)
3. 1 Flattening layer
4. 2 Dropout layers (rate of 0.2)
5. Utilizes Batch Normalization
6. Categorical crossentropy loss function
7. Adam as optimizer (learning rate of 0.0001)

## EfficientNetB3 Transfer Learning Model Architecture
This pre-trained model is a **EfficientNetB3 Transfer Learning Model** which is part of the EfficientNet neural network architectures. It is known for its level of scaling with prioritization of depth, width, and resolution.

Below are the characteristics of the last few layers of the model:

1. 3 Dense layers
2. 3 Activation layers (ReLu for all and Sigmoid for the last)
3. 1 Flattening layer
4. 2 Dropout layers (rate of 0.2)
5. Utilizes Batch Normalization
6. Categorical crossentropy loss function
7. Adam as optimizer (learning rate of 0.0001)

# Results
This section contains the discussion of the results of the model and whether it is fit to be used in practice.

## Convolutional Neural Network (CNN) Model Assessment

### CNN Model Accuracy
Below is the accuracy for the CNN model:

* Training Accuracy: **99.09%**
* Testing Accuracy: **74.37%**

According to the accuracy scores above, the training accuracy is very high (nearly 100%) and the testing accuracy is significantly low which indicates *overfitting*. This occurs when the model is able to fit and train very well on the training data and perform not as well on unseen data because it is not generalized enough. A testing accuracy of 74.37% is not good considering that this is a high-risk classification.

There can be many reasons for this. One is that the dataset is not big enough. A model will benefit from having more data because the accuracy score would not be as sensitive to incorrect classifications. Additionally, the train-test-split was 90/10 which was how the dataset came. Perhaps increasing the test size and decreasing the training size would reduce the overfitting issue because there would be more data for the model to predict than having only 10% of the images. Additionally, the regularization techniques could have been tinkered more with more time and resources.

### CNN Model Loss
Below is the loss for the CNN model:

* Training Loss: **0.04**
* Testing Loss: **2.72**

The same trend can be seen in the training and testing loss. The overfitting issue is again highlighted with the training loss being significantly lower than the testing loss. Therefore, the general issue with this model is overfitting.

### CNN Model Accuracy vs Epochs
![CNN Accuracy vs Epochs](/Users/michellezhang/Desktop/cnn-accuracy.png){width=300}

The accuracy plot for both the training and testing data looks decent. The lines are increasing steadily which indicates that the model is improving as the epochs increase. At the right of the graph, the lines begin to plateau which may indicate a good stopping point because the model is slowing down its improvement and may cause it to overfitting if it was being trained more. Therefore, experimenting with fewer epochs may have increased the performance of the model because of the overfitting issue. Additionally, the distance between the two lines visualizes the overfitting issue.

### CNN Model Loss vs Epochs
![CNN Loss vs Epochs](/Users/michellezhang/Desktop/cnn-loss.png){width=300}

The training loss starts low and keeps decreasing steadily which is good because it indicates that as the number of epochs increase, the loss steadily decreases. This shows that the model is learning the training data and adjusting to minimize the loss. The testing loss, on the other hand, starts low and then increases as the epochs increase in oscillations. The fact that the loss is increasing for the testing data is not good and means that the model is not good at predicting new data. This makes sense because the accuracy is not very high and the steadily decreasing training loss shows that the model may be overfitting to the data.

### CNN Model AdaGram Heatmaps
**Adagram heatmaps** are used as a visualization tool commonly used in natural language processing (NLP) to capture relationships between words. Therefore, for MRI scans, Adagram heatmaps may not directly show much information in the image. MRI scans are not colored, so it's difficult to pinpoint where the model is getting the most information from.

Below are the Adagram Heatmaps for each type of tumor.

![CNN Heatmap for Glioma Tumor](/Users/michellezhang/Desktop/heatmap-glioma.png){width=300}

![CNN Heatmap for Meninglioma Tumor](/Users/michellezhang/Desktop/heatmap-meningioma.png){width=300}

![CNN Heatmap for Pituitary Tumor](/Users/michellezhang/Desktop/heatmap-pituitary.png){width=300}

![CNN Heatmap for No Tumor](/Users/michellezhang/Desktop/heatmap-no-tumor.png){width=300}

Most of the heatmaps are dark purple which represents low intensity from the background of the images. The slight yellow/green/blue parts mostly outline the outter head structure of the MRI scan. There are some dots of these blue colors in the skull which sort of align with where the tumor is. However, the heatmaps do not show much detail about the MRI scans with some activation in certain correct regions.

### CNN Model Confusion Matrix
![CNN Confusion Matrix](/Users/michellezhang/Desktop/cnn-confusion-matrix.png){width=300}

To interpret this confusion matrix, below are calculations of *sensitivity*, the percentage that the model was able to predict true positives, and *specificity*, the percentage that the model was able to correctly predict true negatives.

* Glioma Tumor Sensitivity: **2%**
* Meningioma Tumor Sensitivity: **41.90%**
* Pituitary Tumor Sensitivity: **17.57%**
* No Tumor Sensitivity: **43.48%**

* Glioma Tumor Specificity: **84.77%**
* Meningioma Tumor Specificity: **56.70%**
* Pituitary Tumor Specificity: **63.30%**
* No Tumor Specificity: **71.93%**

The ability that the model predicted a true positive is incredibly low for the glioma tumor. 2% is incredibly low and based on the background, this tumor seemed to be the most difficult to classify out of the three. The best sensitivity were no tumors which means that the model was able to correctly predict that the MRI scan does not have a tumor roughly 44% of the time. These numbers are low which indicates that the model did not perform very well.

The specificity values are much higher which is good. The lowest is predicting meningioma tumors and the best is predicting glioma. These numbers are not too low which is a good sign.

Based on this number, the model struggled more with predicting the specific tumor correctly. Overall, these numbers are not good considering this is a high-risk classification. In the confusion matrix above, the incorrect tumors were predicted more than they should. Additionally, there are many instances of the model predicting that there is no tumor when in fact there is one. This can lead to terrible consequences.

## VGG16 Transfer Learning Model Assessment

### VGG16 Model Accuracy
Below is the accuracy for the VGG16 model:

* Training Accuracy: **94.67%**
* Testing Accuracy: **71.32%**

The analysis of the accuracy is similar to the first model. The training accuracy is very high while the testing accuracy is significantly lower indicating an *overfitting* issue. Since the transfer learning model is likely to be more complex than the first CNN model, perhaps reducing the complexity would be beneficial. Overall, the same solutions mentioned in the first model's performance assessment can be applied to this model.

### VGG16 Model Loss
Below is the loss for the VGG16 model:

* Training Loss: **0.04**
* Testing Loss: **9.69**

Similar to the first model, the overfitting issue can be seen in the loss as well. There is also more of a discrepancy by a lot between these losses compared to the first model. This indicates poorer model performance.

### VGG16 Model Accuracy vs Epochs
![VGG16 Accuracy vs Epochs](/Users/michellezhang/Desktop/vgg16-accuracy.png){width=300}

Both training and testing accuracies increase steadily as the epochs increase. However, the lines are oscillating with peaks everywhere which may indicating that the model has a bit of difficulty learning. Additionally, the test accuracy begins to plateau as the number of epochs increase which is not a good sign. The distance between the lines show the overfitting issue as well.

### VGG16 Model Loss vs Epochs
![VGG16 Loss vs Epochs](/Users/michellezhang/Desktop/vgg16-loss.png){width=300}

The training loss is very low steadily which indicates that the model did a good job learning and minimizing the loss. However, the testing loss has large oscilations and remains stagnant, not steadily increasing or decreasing. The loss reaches heights of 16 to lows of 6, so the range is large which indicates that the model did not do a good job of minimizing the loss for unseen data. This illustrates the overfitting issue where the model does a good job of minimizing the loss in the training, but fails to generalize enough to unseen data leaving the testing loss to be all over the place and not converging.

### VGG16 Transfer Learning Model Confusion Matrix

![VGG16 Confusion Matrix](/Users/michellezhang/Desktop/vgg16-confusion-matrix.png){width=300}

* Glioma Tumor Sensitivity: **8%**
* Meningioma Tumor Sensitivity: **37.14%**
* Pituitary Tumor Sensitivity: **16.22%**
* No Tumor Sensitivity: **43.48%**

* Glioma Tumor Specificity: **99.61%**
* Meningioma Tumor Specificity: **93.64%**
* Pituitary Tumor Specificity: **98.84%**
* No Tumor Specificity: **97.69%**

These numbers are similar to the first model's. The sensitivity is relatively low with the lowest being classifying the glioma tumor, consistent with the first model. Overall, the sensitivties are lower than the first model which means that this model performed worse in terms of classifying the correct tumor.

The specificity is very high for all of the numbers in this model. This indicates that this model did well at distinguishing between the tumors by choosing the tumors that were not present correctly.

It is important to consider both sensitivity and specificity in a high-risk classification. In this case, the sensitivity is low and the specificty is very high and accurate. In this regard, perhaps this model performed better than the first one.

## EfficientNetB3 Transfer Learning Model Assessment

### EfficientNetB3 Model Accuracy
Below is the accuracy for the EfficientNetB3 model:

* Training Accuracy: **99.48%**
* Testing Accuracy: **77.16%**

This accuracy is the best out of the three models. The overfitting issue is still present indicated by the significant gap between the training and testing accuracies. However, the testing accuracy is higher than the other two models which is a good sign.

The ways of tackling the overfitting issue is mentioned extensively in the first model's analysis and applies to this model as well.

### EfficientNetB3 Model Loss
Below is the loss for the EfficientNetB3 model:

* Training Loss: **0.36**
* Testing Loss: **1.23**

Similar to the previous two models, the loss values show that there is still overfitting. However, the testing loss is the lowest than the previous models'. This aligns with the testing accuracy being higher than the others which indicates better model performance.

### EfficientNetB3 Model Accuracy vs Epochs
![EfficientNetB3 Accuracy vs Epochs](/Users/michellezhang/Desktop/efficient-accuracy.png){width=300}

Both accuracies increase steadily as the epochs increase. The training accuracy looks relatively steady and not oscillating meaning that the model is doing a good job of learning as the epochs increase. However, the testing accuracy has large oscillations which illustrates how the model had a difficult time improving the accuracy with the testing data in a steady way.

### EfficientNetB3 Model Loss vs Epochs
![EfficientNetB3 Loss vs Epochs](/Users/michellezhang/Desktop/efficient-loss.png){width=300}

The loss for both training and testing data look good. The training loss is more unform and decreases more than the testing one. Compared to the other loss graphs, this is the most uniform and clean one indicating that the model performed well in terms of minimizing the loss. The testing loss is not as uniform which is expected due to it having fewer images, but overall, the losses look good for this model.

### EfficientNetB3 Transfer Learning Model Confusion Matrix
![EfficientNetB3 Confusion Matrix](/Users/michellezhang/Desktop/efficient-confusion-matrix.png){width=300}

* Glioma Tumor Sensitivity: **13.27%**
* Meningioma Tumor Sensitivity: **39.18%**
* Pituitary Tumor Sensitivity: **17.14%**
* No Tumor Sensitivity: **39.52%**

* Glioma Tumor Specificity: **69.23%**
* Meningioma Tumor Specificity: **69.01%**
* Pituitary Tumor Specificity: **67.16%**
* No Tumor Specificity: **74.39%**

The sensitivity values indicate that the model had a difficult time correctly classifying glioma and pituitary tumors which is consistent with the background of those tumors being more difficult to classify. These values are low and not to be trusted especially considering this is a high-risk classification. The model performed better at classifying meningioma and no tumors but at roughly 40% which is better, but not good enough to be used in practice.

The sensitivity for this model is adequate performing more similar to the first model. The numbers are pretty close and are not terrible.

## In Practice
We believe that none of these models are fit to be used in practice. Firstly, all models had an *overfitting* issue with the training data performing significantly better than the testing data. With a high-risk classification, having a model that does not perform that well on the testing data is not desirable. If the testing data is actual MRI scans for real-life patients, having a wrong diagnosis would be absolutely terrible. As mentioned in the background, each of the tumors have different treatments and life expectancies. It is crucial to have a model that has high accuracy at predicting the type of tumors correctly and minimize the false negatives.

With all of this being said, the best performing model out of the three is the last model (EfficientNetB3 Transfer Learning Model) because it had the lowest loss values, highest accuracy, and best values of confusion matrices. However, these metrics will not suffice in the real world.

# Reflection
This section contains reflections from our group regarding this final project, the code, and technical report.

## Completion Process
The process for completing this final project can be broken down into three sections: Code, Presentation, Technical Report.

### Presentation
As soon as we got a good amount of the code done, we started on the final presentation which did not take too much time. We needed to condense our ideas and report on what we have so far and what we wish to accomplish by the time the final project is due. The work was divided up so that Karen would be working and tinkering with the code because it was difficult to work on the same code on the same file and we thought this would improve our efficiency. Todd and Michelle worked on the presentation and preparing it so that the results from the code could be inserted in. We practiced the presentation a little bit before presentating to make sure our slides flowed easier.

We presented on *Thursday, December 7, 2023* with slides focused on the background, ethical issues data overview, model architectures, model assessment, and issues and future steps. The presentation was short and we tried to communicate everything done so far and especially to stress the importance of our project.

To see our presentation, the click [here](https://www.canva.com/design/DAF2JXwDYDI/g3lRSw1dM2fKZtexb39y2g/view?utm_content=DAF2JXwDYDI&utm_campaign=designshare&utm_medium=link&utm_source=editor).

### Technical Report
The technical report was started at the same time the presentation was. We decided to make a very in-depth technical report and include a large amount of detail to thoroughly explain the problem, analysis, ethical concerns, model architecture and background information, model assessment and results, and reflections to showcase our understanding of the topic and create a cohesive report. The technical report was worked on alongside the code which was improved on since the presentation.

The technical report was written primarily by Michelle with Todd's help and Karen working on the code. Revisiting old lecture videos and doing additional research was necessary to provide a holistic and cohesive report that explored the ideas in-depth.

## Lessons Learned
*Time management* was an essential skill we realized during this project. Since there were so many moving parts to it (the code, slides, technical report, etc.) we knew that we needed to work on the project incrementally and keep each other accountable throughout.

We experimented and used *trial and error* throughout the coding process along with *further research* to tinker with the model in order to create one that performed the best for our dataset. The overfitting issue was a main concern that we faced. After multiple times of tinkering around with the regularization and layers, we came a little bit closer than we did during the presentation, but the issue is still present. We have decided to include an entire section in our technical report discussing this issue and further ways that we could do to mitigate it given more time and resources.

*Teamwork* and *collaboration* drove the project forward. Being able to divide and conquer as well as provide insights and suggestions to each other's work made the process of completing the project more smoothly and easier.

Overall, all of us as a group learned a lot from working together and strive to better ourselves and our understanding of in the field of machine learning and data.
